{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import PngImagePlugin\n",
    "LARGE_ENOUGH_NUMBER = 100\n",
    "PngImagePlugin.MAX_TEXT_CHUNK = LARGE_ENOUGH_NUMBER * (1024**2)\n",
    "import PIL.Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from collections import Counter\n",
    "import multiprocessing\n",
    "num_of_workers = multiprocessing.cpu_count()\n",
    "\n",
    "# cuda visibile devices 0, 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =  \"0, 1\"\n",
    "\n",
    "# Load the data\n",
    "df = pd.read_csv('kevin__data_exploration_taxonomy.csv')\n",
    "\n",
    "def filter_existing_images(df, image_dir):\n",
    "    existing_files = []\n",
    "    for file_name in df['file_name']:\n",
    "        if os.path.exists(os.path.join(image_dir, str(file_name))):\n",
    "            existing_files.append(file_name)\n",
    "    \n",
    "    return df[df['file_name'].isin(existing_files)].reset_index(drop=True)\n",
    "\n",
    "image_dir = \"/home/trkosire/Factorynet/hackathon/all_images/\" \n",
    "df = filter_existing_images(df, image_dir)\n",
    "\n",
    "# set random seed \n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Hyperparameters\n",
    "momentum = 0.9\n",
    "learning_rates = [0.01]\n",
    "learning_rate_decay = 0.0001\n",
    "weight_decay = 0.0002\n",
    "num_epochs = 15\n",
    "batch_sizes = [64]\n",
    "early_stopping_patience = 5 \n",
    "\n",
    "MINORITY_THRESHOLD = 1000\n",
    "# Standard augmentation for majority classes\n",
    "standard_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# More aggressive augmentation for minority classes\n",
    "aggressive_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.2),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, image_dir):\n",
    "        self.df = df\n",
    "        self.image_paths = [os.path.join(image_dir, str(file_name)) for file_name in df['file_name']]\n",
    "        self.labels = df['target'].values\n",
    "        \n",
    "        # Determine minority classes\n",
    "        class_counts = df['target'].value_counts()\n",
    "        self.minority_classes = class_counts[class_counts < MINORITY_THRESHOLD].index.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply appropriate transform based on the class\n",
    "        if label in self.minority_classes:\n",
    "            image = aggressive_transform(image)\n",
    "        else:\n",
    "            image = standard_transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "dataset = CustomDataset(df, image_dir)\n",
    "\n",
    "X = dataset\n",
    "y = dataset.labels\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42)\n",
    "\n",
    "def undersample(X, y):\n",
    "    class_counts = Counter(y)\n",
    "    min_class_count = min(class_counts.values())\n",
    "    \n",
    "    undersampled_indices = []\n",
    "    for class_label in class_counts:\n",
    "        class_indices = np.where(y == class_label)[0]\n",
    "        undersampled_indices.extend(np.random.choice(class_indices, min_class_count, replace=False))\n",
    "    \n",
    "    return [X[i] for i in undersampled_indices], y[undersampled_indices]\n",
    "\n",
    "# Perform undersampling on the training data\n",
    "X_train_undersampled, y_train_undersampled = undersample(X_train, y_train)\n",
    "\n",
    "# Create new datasets\n",
    "train_data_undersampled = CustomDataset(pd.DataFrame({'file_name': [x.image_paths[0].split('/')[-1] for x in X_train_undersampled], 'target': y_train_undersampled}), image_dir)\n",
    "val_data = CustomDataset(pd.DataFrame({'file_name': [x.image_paths[0].split('/')[-1] for x in X_val], 'target': y_val}), image_dir)\n",
    "test_data = CustomDataset(pd.DataFrame({'file_name': [x.image_paths[0].split('/')[-1] for x in X_test], 'target': y_test}), image_dir)\n",
    "\n",
    "# Model setup\n",
    "dropout_rate = 0.5\n",
    "\n",
    "class ModifiedResNet152(torch.nn.Module):\n",
    "    def __init__(self, original_model, dropout_rate):\n",
    "        super(ModifiedResNet152, self).__init__()\n",
    "        self.features = torch.nn.Sequential(*list(original_model.children())[:-1])\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.fc = torch.nn.Linear(original_model.fc.in_features, 11)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "from torchvision.models import ResNet152_Weights\n",
    "original_model = torchvision.models.resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "model = ModifiedResNet152(original_model, dropout_rate)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "# Focal Loss setup\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): \n",
    "            self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list): \n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)\n",
    "            input = input.transpose(1, 2)\n",
    "            input = input.contiguous().view(-1, input.size(2))\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = logpt.exp()\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * at\n",
    "\n",
    "        loss = -1 * (1 - pt)**self.gamma * logpt\n",
    "        if self.size_average: \n",
    "            return loss.mean()\n",
    "        else: \n",
    "            return loss.sum()\n",
    "\n",
    "# Loss function and optimizer setup\n",
    "num_classes = df['target'].nunique()  \n",
    "alpha = torch.ones(num_classes, device=device)\n",
    "class_counts = Counter(y_train_undersampled)\n",
    "total_samples = sum(class_counts.values())\n",
    "for i in range(num_classes):\n",
    "    alpha[i] = 1 - (class_counts[i] / total_samples)\n",
    "criterion = FocalLoss(gamma=2, alpha=alpha)\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0, device=device):\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, test_loader, optimizer, scheduler, num_epochs, early_stopping_patience):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            images, targets_a, targets_b, lam = mixup_data(images, labels, alpha=1.0, device=device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                labels = labels.type(torch.LongTensor)\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                labels = labels.type(torch.LongTensor)\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    # Restore best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model, best_val_loss, train_losses, val_losses, test_losses\n",
    "\n",
    "# Hyperparameter tuning\n",
    "best_hyperparams = {'learning_rate': None, 'batch_size': None, 'val_loss': float('inf')}\n",
    "best_model_state = None\n",
    "best_losses = None\n",
    "\n",
    "results = []\n",
    "\n",
    "for lr in learning_rates:\n",
    "    for bs in batch_sizes:\n",
    "        print(f\"Training with learning rate: {lr}, batch size: {bs}\")\n",
    "\n",
    "        # Create dataloaders\n",
    "        train_loader = DataLoader(train_data_undersampled, batch_size=bs, shuffle=True, num_workers=num_of_workers)\n",
    "        val_loader = DataLoader(val_data, batch_size=bs, shuffle=False,num_workers=num_of_workers)\n",
    "        test_loader = DataLoader(test_data, batch_size=bs, shuffle=False, num_workers=num_of_workers)\n",
    "\n",
    "        # Initialize model\n",
    "        model = ModifiedResNet152(torchvision.models.resnet152(weights=ResNet152_Weights.DEFAULT), dropout_rate=0.5)\n",
    "        model = model.to(device)\n",
    "\n",
    "        # Initialize optimizer and scheduler\n",
    "        optimizer = torch.optim.SGD(model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "        # Train and evaluate\n",
    "        model, val_loss, train_losses, val_losses, test_losses = train_and_evaluate(\n",
    "            model, train_loader, val_loader, test_loader, optimizer, scheduler, num_epochs, early_stopping_patience\n",
    "        )\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'lr': lr,\n",
    "            'bs': bs,\n",
    "            'train_losses': train_losses,\n",
    "            'val_losses': val_losses,\n",
    "            'test_losses': test_losses\n",
    "        })\n",
    "\n",
    "        # Update best hyperparameters if current combination is better\n",
    "        if val_loss < best_hyperparams['val_loss']:\n",
    "            best_hyperparams['learning_rate'] = lr\n",
    "            best_hyperparams['batch_size'] = bs\n",
    "            best_hyperparams['val_loss'] = val_loss\n",
    "            best_model_state = model.state_dict()\n",
    "            best_losses = (train_losses, val_losses, test_losses)\n",
    "\n",
    "print(f\"Best Hyperparameters: Learning Rate = {best_hyperparams['learning_rate']}, Batch Size = {best_hyperparams['batch_size']}\")\n",
    "\n",
    "# Save the best overall model\n",
    "torch.save(best_model_state, 'best_model.pth')\n",
    "\n",
    "# Save the losses for the best model\n",
    "np.save('best_model_losses.npy', best_losses)\n",
    "\n",
    "# Save all results\n",
    "np.save('all_results.npy', results)\n",
    "\n",
    "# Plot learning curves for the best model\n",
    "train_losses, val_losses, test_losses = best_losses\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss', color = 'red')\n",
    "plt.plot(val_losses, label='Validation Loss', color = 'blue')\n",
    "plt.plot(test_losses, label='Test Loss', color = 'green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Learning Curves (LR={best_hyperparams[\"learning_rate\"]}, BS={best_hyperparams[\"batch_size\"]})')\n",
    "plt.legend()\n",
    "plt.savefig('learning_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# Create plots for each hyperparameter combination\n",
    "for result in results:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(result['train_losses'], label='Training Loss', color = 'red')\n",
    "    plt.plot(result['val_losses'], label='Validation Loss', color = 'blue')\n",
    "    plt.plot(result['test_losses'], label='Test Loss', color = 'green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Learning Curves (LR={result[\"lr\"]}, BS={result[\"bs\"]})')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'learning_curves_lr{result[\"lr\"]}_bs{result[\"bs\"]}.png')\n",
    "    plt.close()\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_labels)\n",
    "\n",
    "# Load the best model\n",
    "best_model = ModifiedResNet152(torchvision.models.resnet152(weights=ResNet152_Weights.DEFAULT), dropout_rate=0.5)\n",
    "best_model = best_model.to(device)\n",
    "best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Create test DataLoader\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Get predictions\n",
    "predictions, true_labels = evaluate_model(best_model, test_loader, device)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Calculate per-class metrics\n",
    "class_precision, class_recall, class_f1, _ = precision_recall_fscore_support(true_labels, predictions, average=None)\n",
    "\n",
    "# Print per-class metrics\n",
    "for i in range(len(class_precision)):\n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"  Precision: {class_precision[i]:.4f}\")\n",
    "    print(f\"  Recall: {class_recall[i]:.4f}\")\n",
    "    print(f\"  F1 Score: {class_f1[i]:.4f}\")\n",
    "\n",
    "# Create a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()\n",
    "\n",
    "# Print class distribution after undersampling\n",
    "print(\"\\nClass distribution after undersampling:\")\n",
    "class_counts = Counter(y_train_undersampled)\n",
    "total_samples = sum(class_counts.values())\n",
    "for class_label, count in sorted(class_counts.items()):\n",
    "    print(f\"Class {class_label}: {count} samples ({count/total_samples:.2%})\")\n",
    "\n",
    "# You might want to add more analysis or visualization here\n",
    "# For example, you could plot the distribution of classes after undersampling\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(class_counts.keys(), class_counts.values())\n",
    "plt.title('Class Distribution After Undersampling')\n",
    "plt.xlabel('Class')\n",
    "plt.ylabel('Number of Samples')\n",
    "plt.savefig('class_distribution_after_undersampling.png')\n",
    "plt.close()\n",
    "\n",
    "print(\"\\nTraining completed. Results and visualizations have been saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet18, ResNet18_Weights\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "from PIL import PngImagePlugin\n",
    "LARGE_ENOUGH_NUMBER = 100\n",
    "PngImagePlugin.MAX_TEXT_CHUNK = LARGE_ENOUGH_NUMBER * (1024**2)\n",
    "import PIL.Image\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "# cuda visibile devices 0, 1\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] =  \"0, 1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "df = pd.read_csv('kevin__data_exploration_taxonomy.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('kevin__data_exploration_taxonomy.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_existing_images(df, image_dir):\n",
    "    existing_files = []\n",
    "    for file_name in df['file_name']:\n",
    "        if os.path.exists(os.path.join(image_dir, str(file_name))):\n",
    "            existing_files.append(file_name)\n",
    "    \n",
    "    return df[df['file_name'].isin(existing_files)].reset_index(drop=True)\n",
    "\n",
    "image_dir = \"/home/trkosire/Factorynet/hackathon/all_images/\" \n",
    "df = filter_existing_images(df, image_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>x1</th>\n",
       "      <th>y1</th>\n",
       "      <th>height</th>\n",
       "      <th>width</th>\n",
       "      <th>source</th>\n",
       "      <th>file_name</th>\n",
       "      <th>analyzed_labels</th>\n",
       "      <th>taxonomy_label</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>railway grinding machine</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>user</td>\n",
       "      <td>0_1711591656377_railway grinding machine.png</td>\n",
       "      <td>['railway', 'grinding', 'machine']</td>\n",
       "      <td>materials</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rail grinder</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>user</td>\n",
       "      <td>1_1711591656377_rail grinder.png</td>\n",
       "      <td>['rail', 'grinder']</td>\n",
       "      <td>materials</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rail grinder with worker</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>user</td>\n",
       "      <td>2_1711591656377_rail grinder with worker.png</td>\n",
       "      <td>['rail', 'grinder', 'with', 'worker']</td>\n",
       "      <td>materials</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>railway grinding machine with worker</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>user</td>\n",
       "      <td>3_1711591656377_railway grinding machine with ...</td>\n",
       "      <td>['railway', 'grinding', 'machine', 'with', 'wo...</td>\n",
       "      <td>materials</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>worker</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>user</td>\n",
       "      <td>4_1711591656377_worker.png</td>\n",
       "      <td>['worker']</td>\n",
       "      <td>maintenance</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23288</th>\n",
       "      <td>minibar t</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2660.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>wikimedia</td>\n",
       "      <td>23296_1711580020679_minibar t.png</td>\n",
       "      <td>['minibar', 't']</td>\n",
       "      <td>machines</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23289</th>\n",
       "      <td>rail clamp</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>605.0</td>\n",
       "      <td>907.0</td>\n",
       "      <td>user</td>\n",
       "      <td>23297_1711581093292_rail clamp.png</td>\n",
       "      <td>['rail', 'clamp']</td>\n",
       "      <td>materials</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23290</th>\n",
       "      <td>patrol rail fastening</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2304.0</td>\n",
       "      <td>3456.0</td>\n",
       "      <td>wikimedia</td>\n",
       "      <td>23298_1711581093292_patrol rail fastening.png</td>\n",
       "      <td>['patrol', 'rail', 'fastening']</td>\n",
       "      <td>safety</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23291</th>\n",
       "      <td>drill bit</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>651.0</td>\n",
       "      <td>907.0</td>\n",
       "      <td>user</td>\n",
       "      <td>23299_1711566352395_drill bit.png</td>\n",
       "      <td>['drill', 'bit']</td>\n",
       "      <td>machines</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23292</th>\n",
       "      <td>percussion drill</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>1062.0</td>\n",
       "      <td>wikimedia</td>\n",
       "      <td>23300_1711566352395_percussion drill.png</td>\n",
       "      <td>['percussion', 'drill']</td>\n",
       "      <td>specialized</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23293 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      label   x1   y1  height   width  \\\n",
       "0                  railway grinding machine  0.0  0.0   720.0   720.0   \n",
       "1                              rail grinder  0.0  0.0   720.0   720.0   \n",
       "2                  rail grinder with worker  0.0  0.0   720.0   720.0   \n",
       "3      railway grinding machine with worker  0.0  0.0   720.0   720.0   \n",
       "4                                    worker  0.0  0.0   720.0   720.0   \n",
       "...                                     ...  ...  ...     ...     ...   \n",
       "23288                             minibar t  0.0  0.0  2660.0  4000.0   \n",
       "23289                            rail clamp  0.0  0.0   605.0   907.0   \n",
       "23290                 patrol rail fastening  0.0  0.0  2304.0  3456.0   \n",
       "23291                             drill bit  0.0  0.0   651.0   907.0   \n",
       "23292                      percussion drill  0.0  0.0   762.0  1062.0   \n",
       "\n",
       "          source                                          file_name  \\\n",
       "0           user       0_1711591656377_railway grinding machine.png   \n",
       "1           user                   1_1711591656377_rail grinder.png   \n",
       "2           user       2_1711591656377_rail grinder with worker.png   \n",
       "3           user  3_1711591656377_railway grinding machine with ...   \n",
       "4           user                         4_1711591656377_worker.png   \n",
       "...          ...                                                ...   \n",
       "23288  wikimedia                  23296_1711580020679_minibar t.png   \n",
       "23289       user                 23297_1711581093292_rail clamp.png   \n",
       "23290  wikimedia      23298_1711581093292_patrol rail fastening.png   \n",
       "23291       user                  23299_1711566352395_drill bit.png   \n",
       "23292  wikimedia           23300_1711566352395_percussion drill.png   \n",
       "\n",
       "                                         analyzed_labels taxonomy_label  \\\n",
       "0                     ['railway', 'grinding', 'machine']      materials   \n",
       "1                                    ['rail', 'grinder']      materials   \n",
       "2                  ['rail', 'grinder', 'with', 'worker']      materials   \n",
       "3      ['railway', 'grinding', 'machine', 'with', 'wo...      materials   \n",
       "4                                             ['worker']    maintenance   \n",
       "...                                                  ...            ...   \n",
       "23288                                   ['minibar', 't']       machines   \n",
       "23289                                  ['rail', 'clamp']      materials   \n",
       "23290                    ['patrol', 'rail', 'fastening']         safety   \n",
       "23291                                   ['drill', 'bit']       machines   \n",
       "23292                            ['percussion', 'drill']    specialized   \n",
       "\n",
       "       target  \n",
       "0           4  \n",
       "1           4  \n",
       "2           4  \n",
       "3           4  \n",
       "4           3  \n",
       "...       ...  \n",
       "23288       2  \n",
       "23289       4  \n",
       "23290       8  \n",
       "23291       2  \n",
       "23292       9  \n",
       "\n",
       "[23293 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of workers: 48\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_workers = multiprocessing.cpu_count()\n",
    "\n",
    "print(f\"Number of workers: {num_workers}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set random seed \n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# # Hyperparameters\n",
    "momentum = 0.9\n",
    "learning_rates = 0.01 # best learning rate is 0.01\n",
    "learning_rate_decay = 0.0001\n",
    "weight_decay = 0.0002\n",
    "num_epochs = 15\n",
    "batch_sizes = 64 # best batch size is 128\n",
    "early_stopping_patience = 5 \n",
    "\n",
    "\n",
    "MINORITY_THRESHOLD = 1000\n",
    "# Standard augmentation for majority classes\n",
    "standard_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=15),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# More aggressive augmentation for minority classes\n",
    "aggressive_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.RandomRotation(degrees=30),\n",
    "    transforms.ColorJitter(brightness=0.5, contrast=0.5, saturation=0.5, hue=0.2),\n",
    "    transforms.RandomAffine(degrees=0, translate=(0.1, 0.1), scale=(0.8, 1.2), shear=10),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "\n",
    "# create a dataset for each cluster\n",
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, image_dir):\n",
    "        self.df = df\n",
    "        self.image_paths = [os.path.join(image_dir, str(file_name)) for file_name in df['file_name']]\n",
    "        self.labels = df['target'].values\n",
    "        \n",
    "        # Determine minority classes\n",
    "        class_counts = df['target'].value_counts()\n",
    "        self.minority_classes = class_counts[class_counts < MINORITY_THRESHOLD].index.tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path = self.image_paths[idx]\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Apply appropriate transform based on the class\n",
    "        if label in self.minority_classes:\n",
    "            image = aggressive_transform(image)\n",
    "        else:\n",
    "            image = standard_transform(image)\n",
    "        \n",
    "        return image, label\n",
    "\n",
    "    \n",
    "dataset = CustomDataset(df, image_dir)\n",
    "\n",
    "X = dataset\n",
    "y = dataset.labels\n",
    "\n",
    "# # Get image paths and labels for oversampling\n",
    "# X = dataset.image_paths  # Using image paths as a placeholder for X\n",
    "# y = dataset.labels\n",
    "\n",
    "# from imblearn.over_sampling import RandomOverSampler\n",
    "# # Apply Random Over Sampler\n",
    "# ros = RandomOverSampler(random_state=42)\n",
    "# X_resampled, y_resampled = ros.fit_resample([[x] for x in X], y)  # X needs to be 2D\n",
    "# X_resampled = [x[0] for x in X_resampled]  # Flatten the list\n",
    "\n",
    "# X = X_resampled\n",
    "# y = y_resampled\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.25, stratify=y_train_val, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save X_train, X_val, X_test, y_train, y_val, y_test to disk\n",
    "# torch.save(X_train, 'X_train.pt')\n",
    "# torch.save(X_val, 'X_val.pt')\n",
    "# torch.save(X_test, 'X_test.pt')\n",
    "# torch.save(y_train, 'y_train.pt')\n",
    "# torch.save(y_val, 'y_val.pt')\n",
    "# torch.save(y_test, 'y_test.pt')\n",
    "\n",
    "# Load the data\n",
    "# X_train = torch.load('X_train.pt')\n",
    "# X_val = torch.load('X_val.pt')\n",
    "# X_test = torch.load('X_test.pt')\n",
    "# y_train = torch.load('y_train.pt')\n",
    "# y_val = torch.load('y_val.pt')\n",
    "# y_test = torch.load('y_test.pt')\n",
    "\n",
    "\n",
    "# train_data = ConcatDataset([X_train])\n",
    "# val_data = ConcatDataset([X_val])\n",
    "# test_data = ConcatDataset([X_test])\n",
    "\n",
    "train_data = CustomDataset(pd.DataFrame({'file_name': X_train, 'target': y_train}), image_dir)\n",
    "val_data = CustomDataset(pd.DataFrame({'file_name': X_val, 'target': y_val}), image_dir)\n",
    "test_data = CustomDataset(pd.DataFrame({'file_name': X_test, 'target': y_test}), image_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set class distribution:\n",
      "Class 0: 2301\n",
      "Class 1: 2300\n",
      "Class 2: 2301\n",
      "Class 3: 2300\n",
      "Class 4: 2300\n",
      "Class 5: 2301\n",
      "Class 6: 2300\n",
      "Class 7: 2300\n",
      "Class 8: 2300\n",
      "Class 9: 2300\n",
      "Class 10: 2301\n",
      "\n",
      "Validation set class distribution:\n",
      "Class 0: 767\n",
      "Class 1: 767\n",
      "Class 2: 766\n",
      "Class 3: 767\n",
      "Class 4: 767\n",
      "Class 5: 767\n",
      "Class 6: 767\n",
      "Class 7: 767\n",
      "Class 8: 767\n",
      "Class 9: 767\n",
      "Class 10: 766\n",
      "\n",
      "Test set class distribution:\n",
      "Class 0: 766\n",
      "Class 1: 767\n",
      "Class 2: 767\n",
      "Class 3: 767\n",
      "Class 4: 767\n",
      "Class 5: 766\n",
      "Class 6: 767\n",
      "Class 7: 767\n",
      "Class 8: 767\n",
      "Class 9: 767\n",
      "Class 10: 767\n",
      "\n",
      "Percentage distribution:\n",
      "Class\tTrain\tVal\tTest\n",
      "0\t9.09%\t9.09%\t9.08%\n",
      "1\t9.09%\t9.09%\t9.09%\n",
      "2\t9.09%\t9.08%\t9.09%\n",
      "3\t9.09%\t9.09%\t9.09%\n",
      "4\t9.09%\t9.09%\t9.09%\n",
      "5\t9.09%\t9.09%\t9.08%\n",
      "6\t9.09%\t9.09%\t9.09%\n",
      "7\t9.09%\t9.09%\t9.09%\n",
      "8\t9.09%\t9.09%\t9.09%\n",
      "9\t9.09%\t9.09%\t9.09%\n",
      "10\t9.09%\t9.08%\t9.09%\n"
     ]
    }
   ],
   "source": [
    "# print the distribution of the classes in the train, val and test set\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Assuming y_train, y_val, and y_test are your label arrays for each set\n",
    "\n",
    "# Count occurrences of each class\n",
    "train_distribution = Counter(y_train)\n",
    "val_distribution = Counter(y_val)\n",
    "test_distribution = Counter(y_test)\n",
    "\n",
    "# Print distributions\n",
    "print(\"Train set class distribution:\")\n",
    "for class_label, count in sorted(train_distribution.items()):\n",
    "    print(f\"Class {class_label}: {count}\")\n",
    "\n",
    "print(\"\\nValidation set class distribution:\")\n",
    "for class_label, count in sorted(val_distribution.items()):\n",
    "    print(f\"Class {class_label}: {count}\")\n",
    "\n",
    "print(\"\\nTest set class distribution:\")\n",
    "for class_label, count in sorted(test_distribution.items()):\n",
    "    print(f\"Class {class_label}: {count}\")\n",
    "\n",
    "# Calculate percentages\n",
    "total_train = sum(train_distribution.values())\n",
    "total_val = sum(val_distribution.values())\n",
    "total_test = sum(test_distribution.values())\n",
    "\n",
    "print(\"\\nPercentage distribution:\")\n",
    "print(\"Class\\tTrain\\tVal\\tTest\")\n",
    "for class_label in sorted(set(train_distribution.keys()) | set(val_distribution.keys()) | set(test_distribution.keys())):\n",
    "    train_percent = train_distribution[class_label] / total_train * 100\n",
    "    val_percent = val_distribution[class_label] / total_val * 100\n",
    "    test_percent = test_distribution[class_label] / total_test * 100\n",
    "    print(f\"{class_label}\\t{train_percent:.2f}%\\t{val_percent:.2f}%\\t{test_percent:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Model setup\n",
    "dropout_rate = 0.5\n",
    "\n",
    "class ModifiedResNet152(torch.nn.Module):\n",
    "    def __init__(self, original_model, dropout_rate):\n",
    "        super(ModifiedResNet152, self).__init__()\n",
    "        self.features = torch.nn.Sequential(*list(original_model.children())[:-1])\n",
    "        self.dropout = torch.nn.Dropout(dropout_rate)\n",
    "        self.fc = torch.nn.Linear(original_model.fc.in_features, 11)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "from torchvision.models import ResNet152_Weights\n",
    "original_model = torchvision.models.resnet152(weights=ResNet152_Weights.DEFAULT)\n",
    "model = ModifiedResNet152(original_model, dropout_rate)\n",
    "model = torch.nn.DataParallel(model)\n",
    "model = model.to(device)\n",
    "\n",
    "# due to class imbalance we will use weighted cross entropy loss\n",
    "class_counts = df['target'].value_counts()\n",
    "class_weights = np.zeros(len(class_counts))\n",
    "for class_index, count in class_counts.items():\n",
    "    class_weights[class_index] = 1.0 / count\n",
    "class_weights = class_weights / class_weights.sum()\n",
    "class_weights = torch.FloatTensor(class_weights).to(device)\n",
    "\n",
    "class FocalLoss(torch.nn.Module):\n",
    "    def __init__(self, gamma=2, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)): \n",
    "            self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list): \n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            input = input.view(input.size(0), input.size(1), -1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1, 2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))   # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = logpt.exp()\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * at\n",
    "\n",
    "        loss = -1 * (1 - pt)**self.gamma * logpt\n",
    "        if self.size_average: \n",
    "            return loss.mean()\n",
    "        else: \n",
    "            return loss.sum()\n",
    "        \n",
    "        \n",
    "# Loss function and optimizer\n",
    "num_classes = df['target'].nunique()  \n",
    "alpha = torch.ones(num_classes, device=device)\n",
    "alpha = alpha * (1 - class_weights)  \n",
    "criterion = FocalLoss(gamma=2, alpha=alpha)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rates, momentum=momentum, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=learning_rate_decay)\n",
    "\n",
    "\n",
    "\n",
    "def mixup_data(x, y, alpha=1.0, device=device):\n",
    "    '''Compute the mixup data. Return mixed inputs, pairs of targets, and lambda'''\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index, :]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    '''Compute the mixup loss'''\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, test_loader, optimizer, scheduler, num_epochs, early_stopping_patience):\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for images, labels in train_loader:\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            images, targets_a, targets_b, lam = mixup_data(images, labels, alpha=1.0, device=device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = mixup_criterion(criterion, outputs, targets_a, targets_b, lam)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # Evaluate on validation set\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in val_loader:\n",
    "                labels = labels.type(torch.LongTensor)\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        # Evaluate on test set\n",
    "        test_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, labels in test_loader:\n",
    "                labels = labels.type(torch.LongTensor)\n",
    "                images, labels = images.to(device), labels.to(device)\n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "        avg_test_loss = test_loss / len(test_loader)\n",
    "\n",
    "        train_losses.append(avg_train_loss)\n",
    "        val_losses.append(avg_val_loss)\n",
    "        test_losses.append(avg_test_loss)\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Test Loss: {avg_test_loss:.4f}')\n",
    "\n",
    "        # Early stopping check\n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= early_stopping_patience:\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    # Restore best model\n",
    "    model.load_state_dict(best_model_state)\n",
    "    return model, best_val_loss, train_losses, val_losses, test_losses\n",
    "\n",
    "# Hyperparameter tuning\n",
    "best_hyperparams = {'learning_rate': None, 'batch_size': None, 'val_loss': float('inf')}\n",
    "best_model_state = None\n",
    "best_losses = None\n",
    "\n",
    "results = []\n",
    "\n",
    "# for lr in learning_rates:\n",
    "#     for bs in batch_sizes:\n",
    "print(f\"Training with learning rate: {learning_rates}, batch size: {batch_sizes}\")\n",
    "\n",
    "# Create dataloaders\n",
    "train_loader = DataLoader(train_data, batch_size=batch_sizes, shuffle=True, num_workers= num_workers)\n",
    "val_loader = DataLoader(val_data, batch_size=batch_sizes, shuffle=False, num_workers = num_workers)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_sizes, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "# Initialize model\n",
    "model = ModifiedResNet152(torchvision.models.resnet152(weights=ResNet152_Weights.DEFAULT), dropout_rate=0.5)\n",
    "model = model.to(device)\n",
    "\n",
    "# Initialize optimizer and scheduler\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rates, momentum=momentum, weight_decay=weight_decay)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# Train and evaluate\n",
    "model, val_loss, train_losses, val_losses, test_losses = train_and_evaluate(\n",
    "    model, train_loader, val_loader, test_loader, optimizer, scheduler, num_epochs, early_stopping_patience\n",
    ")\n",
    "\n",
    "# Store results\n",
    "results.append({\n",
    "    'lr': learning_rates,\n",
    "    'bs': batch_sizes,\n",
    "    'train_losses': train_losses,\n",
    "    'val_losses': val_losses,\n",
    "    'test_losses': test_losses\n",
    "})\n",
    "\n",
    "# Update best hyperparameters if current combination is better\n",
    "if val_loss < best_hyperparams['val_loss']:\n",
    "    best_hyperparams['learning_rate'] = learning_rates\n",
    "    best_hyperparams['batch_size'] = batch_sizes\n",
    "    best_hyperparams['val_loss'] = val_loss\n",
    "    best_model_state = model.state_dict()\n",
    "    best_losses = (train_losses, val_losses, test_losses)\n",
    "\n",
    "print(f\"Best Hyperparameters: Learning Rate = {best_hyperparams['learning_rate']}, Batch Size = {best_hyperparams['batch_size']}\")\n",
    "\n",
    "# Save the best overall model\n",
    "torch.save(best_model_state, 'best_model.pth')\n",
    "\n",
    "# Save the losses for the best model\n",
    "np.save('best_model_losses.npy', best_losses)\n",
    "\n",
    "# Save all results\n",
    "np.save('all_results.npy', results)\n",
    "\n",
    "# Plot learning curves for the best model\n",
    "train_losses, val_losses, test_losses = best_losses\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss', color = 'red')\n",
    "plt.plot(val_losses, label='Validation Loss', color = 'blue')\n",
    "plt.plot(test_losses, label='Test Loss', color = 'green')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(f'Learning Curves (LR={best_hyperparams[\"learning_rate\"]}, BS={best_hyperparams[\"batch_size\"]})')\n",
    "plt.legend()\n",
    "plt.savefig('learning_curves.png')\n",
    "plt.close()\n",
    "\n",
    "# Create plots for each hyperparameter combination\n",
    "for result in results:\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(result['train_losses'], label='Training Loss', color = 'red')\n",
    "    plt.plot(result['val_losses'], label='Validation Loss', color = 'blue')\n",
    "    plt.plot(result['test_losses'], label='Test Loss', color = 'green')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Learning Curves (LR={result[\"lr\"]}, BS={result[\"bs\"]})')\n",
    "    plt.legend()\n",
    "    plt.savefig(f'learning_curves_lr{result[\"lr\"]}_bs{result[\"bs\"]}.png')\n",
    "    plt.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# # Save the training dataset\n",
    "# train_data = [(image, label) for image, label in train_loader.dataset]\n",
    "# torch.save(train_data, 'train_data.pt')\n",
    "\n",
    "# # Save the validation dataset\n",
    "# val_data = [(image, label) for image, label in val_loader.dataset]\n",
    "# torch.save(val_data, 'val_data.pt')\n",
    "\n",
    "# # Save the test dataset\n",
    "# test_data = [(image, label) for image, label in test_loader.dataset]\n",
    "# torch.save(test_data, 'test_data.pt')\n",
    "\n",
    "# # Load the training dataset\n",
    "# train_data = torch.load('train_data.pt')\n",
    "\n",
    "# # Load the validation dataset\n",
    "# val_data = torch.load('val_data.pt')\n",
    "\n",
    "# # Load the test dataset\n",
    "# test_data = torch.load('test_data.pt')\n",
    "\n",
    "\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# # Convert lists of tuples back to datasets\n",
    "# class CustomDataset(torch.utils.data.Dataset):\n",
    "#     def __init__(self, data):\n",
    "#         self.data = data\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         image, label = self.data[idx]\n",
    "#         return image, label\n",
    "\n",
    "# # Recreate datasets\n",
    "# train_dataset = CustomDataset(train_data)\n",
    "# val_dataset = CustomDataset(val_data)\n",
    "# test_dataset = CustomDataset(test_data)\n",
    "\n",
    "# # Recreate DataLoaders\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_sizes, shuffle=True, num_workers=num_workers)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_sizes, shuffle=False, num_workers=num_workers)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_sizes, shuffle=False, num_workers=num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2031983/3257363441.py:28: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  best_model.load_state_dict(torch.load('best_model.pth'))\n",
      "/home/trkosire/Factorynet/hack_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/trkosire/Factorynet/hack_env/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3003\n",
      "Precision: 0.2970\n",
      "Recall: 0.3003\n",
      "F1 Score: 0.2889\n",
      "Class 0:\n",
      "  Precision: 0.2803\n",
      "  Recall: 0.1995\n",
      "  F1 Score: 0.2331\n",
      "Class 1:\n",
      "  Precision: 0.4352\n",
      "  Recall: 0.4309\n",
      "  F1 Score: 0.4330\n",
      "Class 2:\n",
      "  Precision: 0.2890\n",
      "  Recall: 0.3455\n",
      "  F1 Score: 0.3147\n",
      "Class 3:\n",
      "  Precision: 0.2527\n",
      "  Recall: 0.1551\n",
      "  F1 Score: 0.1922\n",
      "Class 4:\n",
      "  Precision: 0.2588\n",
      "  Recall: 0.1746\n",
      "  F1 Score: 0.2085\n",
      "Class 5:\n",
      "  Precision: 0.0000\n",
      "  Recall: 0.0000\n",
      "  F1 Score: 0.0000\n",
      "Class 6:\n",
      "  Precision: 0.2644\n",
      "  Recall: 0.2644\n",
      "  F1 Score: 0.2644\n",
      "Class 7:\n",
      "  Precision: 0.2464\n",
      "  Recall: 0.1857\n",
      "  F1 Score: 0.2118\n",
      "Class 8:\n",
      "  Precision: 0.3125\n",
      "  Recall: 0.1203\n",
      "  F1 Score: 0.1737\n",
      "Class 9:\n",
      "  Precision: 0.5289\n",
      "  Recall: 0.6575\n",
      "  F1 Score: 0.5862\n",
      "Class 10:\n",
      "  Precision: 0.2652\n",
      "  Recall: 0.4499\n",
      "  F1 Score: 0.3337\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            labels = labels.type(torch.LongTensor)\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    return np.array(all_predictions), np.array(all_labels)\n",
    "\n",
    "# Load the best model\n",
    "best_model = ModifiedResNet152(torchvision.models.resnet152(weights=ResNet152_Weights.DEFAULT), dropout_rate=0.5)\n",
    "best_model = best_model.to(device)\n",
    "best_model.load_state_dict(torch.load('best_model.pth'))\n",
    "\n",
    "# Create test DataLoader\n",
    "test_loader = DataLoader(test_data, batch_size=64, shuffle=False)\n",
    "\n",
    "# Get predictions\n",
    "predictions, true_labels = evaluate_model(best_model, test_loader, device)\n",
    "\n",
    "# Calculate metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "# Calculate per-class metrics\n",
    "class_precision, class_recall, class_f1, _ = precision_recall_fscore_support(true_labels, predictions, average=None)\n",
    "\n",
    "# Print per-class metrics\n",
    "for i in range(len(class_precision)):\n",
    "    print(f\"Class {i}:\")\n",
    "    print(f\"  Precision: {class_precision[i]:.4f}\")\n",
    "    print(f\"  Recall: {class_recall[i]:.4f}\")\n",
    "    print(f\"  F1 Score: {class_f1[i]:.4f}\")\n",
    "\n",
    "\n",
    "# Create a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(true_labels, predictions)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.savefig('confusion_matrix.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print distribution of true labels and predicted labels\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.hist(true_labels, bins=12, alpha=0.5, label='True Labels', color='blue')\n",
    "plt.hist(predictions, bins=12, alpha=0.5, label='Predicted Labels', color='red')\n",
    "plt.xlabel('Label')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Distribution of True and Predicted Labels')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hack_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
